{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with Deep Learning models using GluonTS\n",
    "\n",
    "Case study via the [M5 forecasting competition dataset](https://www.kaggle.com/competitions/m5-forecasting-accuracy).\n",
    "\n",
    "M-competitions named after Spyros Makridakis, currently in their [6th edition](https://mofc.unic.ac.cy/the-m6-competition/).\n",
    "\n",
    "M5 data provided by Walmart.\n",
    "\n",
    "We assume the data set is downloaded locally (we can't provide it for Kaggle licensing)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M5 dataset\n",
    "\n",
    "* 42,840 hierarchical time series, 3049 products from 3 categories, 7 departments\n",
    "\n",
    "* 3 US states: California (CA), Texas (TX), and Wisconsin (WI), 10 stores\n",
    "\n",
    "* “Hierarchical” levels: item level, department level, product category level, and state level.\n",
    "\n",
    "* Daily sales: Jan 2011 to June 2016. \n",
    "\n",
    "* included co-variates: prices, promotions, and holidays. \n",
    "\n",
    "* no missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We use mainly standard pandas to load and manipulate data, for GluonTS models to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from gluonts.dataset.pandas import PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.read_csv(\n",
    "    Path(\"m5-forecasting-accuracy\") / \"calendar.csv\",\n",
    ")\n",
    "weekly_prices = pd.read_csv(\n",
    "    Path(\"m5-forecasting-accuracy\") / \"sell_prices.csv\",\n",
    ")\n",
    "sales_and_features = pd.read_csv(\n",
    "    Path(\"m5-forecasting-accuracy\") / \"sales_train_validation.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sales_and_features[\"item_id\"].unique()) == 3049\n",
    "assert len(sales_and_features[\"store_id\"].unique()) == 10\n",
    "assert len(sales_and_features) == 30490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_and_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split the data into static (categorical features) vs dynamic (sales data). We keep the 'id' column in both, to be able to join the two. We also keep 'item_id' and 'store_id' in the sales data, to be able to join with prices later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [\"id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "sales_columns = [\"id\", \"item_id\", \"store_id\"] + [f\"d_{k}\" for k in range(1, 1914)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into static (categorical features) vs dynamic (sales data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sales_and_features[features_columns].set_index(\"id\").astype(\"category\")\n",
    "sales = sales_and_features[sales_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(features) == 30490\n",
    "assert len(features.columns) == 4\n",
    "assert len(sales) == 30490\n",
    "assert len(sales.columns) == 1916"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn sales data into long format, to join with prices more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_long = sales.melt(id_vars=[\"id\", \"item_id\", \"store_id\"], var_name=\"d\", value_name=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_prices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join sales data with prices, first we add the `\"wm_yr_wk\"` column from `cal`. We also add the `\"date\"` column to build the time index. Then we join with `weekly_prices` on `\"store_id\"`, `\"item_id\"`, `\"wm_yr_wk\"`, to get the `\"sell_price\"` column in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = sales_long.merge(\n",
    "    cal[[\"d\", \"wm_yr_wk\", \"date\"]], on=\"d\", how=\"left\", suffixes=(None, \"_right\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_prices = temp.merge(weekly_prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\", suffixes=(None, \"_right\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_prices.index = pd.to_datetime(sales_with_prices[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sales_with_prices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows have missing price, which means the item was not for sale. Let's replace price there with some constant, and add a column indicating whether the product was for sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_prices[\"for_sale\"] = 1.0 * sales_with_prices[\"sell_price\"].notna()\n",
    "sales_with_prices[\"sell_price\"].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_with_prices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to construct our dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PandasDataset.from_long_dataframe(\n",
    "    sales_with_prices,\n",
    "    item_id=\"id\",\n",
    "    target=\"sales\",\n",
    "    feat_dynamic_real=[\"sell_price\", \"for_sale\"],\n",
    "    static_features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for entry in tqdm(dataset):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A transformer model\n",
    "\n",
    "We will train a transformer-based architecture (temporal fusion transformer, TFT?) on the above data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stellalo/.pyenv/versions/gluonts/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TemporalFusionTransformerEstimator(\n",
    "    freq=\"1D\",\n",
    "    prediction_length=7,\n",
    "    context_length=30,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting, evaluating, comparing\n",
    "\n",
    "We will plot forecasts, evaluate accuracy and identify worst-cases, compare models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Tuning the model hyperparameters (a.g. architectural choices, number of layers, hidden layers sizes, etc.) is often important to get the best results. GluonTS does not provide model tuning features out of the box, but interfaces easily for dedicated toolboxes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for experiments\n",
    "\n",
    "Besides the specific M5 use-case above, it is important to validate the performance of a model class against multiple datasets. This is especially true when working on novel architectures, or adapting architectures from other domains (NLP, computer vision) to time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluonts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ece9bdd984f8cd41222b51b2510f420829ccf1d6b5fba4524095ddc78fa60611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
